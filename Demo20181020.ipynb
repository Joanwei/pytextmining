{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to format my hard disk', ' Hard disk format problems ']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算查詢關鍵字對文章的距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.txt  02.txt  03.txt  04.txt  05.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls data/toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/toy/01.txt',\n",
       " 'data/toy/02.txt',\n",
       " 'data/toy/03.txt',\n",
       " 'data/toy/04.txt',\n",
       " 'data/toy/05.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path  = 'data/toy'\n",
    "[os.path.join(path, f) for f in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [open(os.path.join(path, f)).read() for f in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "# Tell you how many unique words\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features)) \n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "# Tell you how many unique words\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算查詢關鍵字對文章的距離 (距離一：歐式距離)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy \n",
    "a = numpy.array([0,0,1,1,0])\n",
    "b = numpy.array([1,0,1,0,0])\n",
    "dist_raw(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "a = [0,0,1,1,0]\n",
    "b = [1,0,1,0,0]\n",
    "d = []\n",
    "for i in range(0,len(a)):\n",
    "    d.append(a[i] - b[i])\n",
    "math.sqrt(sum([ele ** 2for ele in d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = numpy.array([0,0,1,1,0])\n",
    "b = numpy.array([1,0,1,0,0])\n",
    "math.sqrt(sum((a - b)** 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = 999\n",
    "best_i = None\n",
    "num_samples = len(posts)\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post==new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This is a toy post about machine learning. Actually, it contains not much interesting stuff. 3.872983346207417\n",
      "1 Imaging databases provide storage capabilities. 2.0\n",
      "2 Most imaging databases safe images permanently. 2.23606797749979\n",
      "3 Imaging databases store data. 1.7320508075688772\n",
      "4 Imaging databases store data. Imaging databases store data. Imaging databases store data. 5.5677643628300215\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i, posts[i], dist_raw(X_train[i], new_post_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算查詢關鍵字對文章的距離 (距離二：標準化後的歐式距離)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1.toarray()) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57735027, 0.57735027, 0.57735027, 0.        , 0.        ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = numpy.array([1,1,1,0,0])\n",
    "#math.sqrt(3)\n",
    "a / sp.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57735027, 0.57735027, 0.57735027, 0.        , 0.        ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = numpy.array([3,3,3,0,0])\n",
    "b / sp.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "1 Imaging databases provide storage capabilities. 1.0514622242382672\n",
      "2 Most imaging databases safe images permanently. 1.0878894332937856\n",
      "3 Imaging databases store data. 1.0\n",
      "4 Imaging databases store data. Imaging databases store data. Imaging databases store data. 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i, posts[i], dist(X_train[i], new_post_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "# Tell you how many unique words\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移除停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'toy']\n",
      "(5, 18)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "1 Imaging databases provide storage capabilities. 1.0514622242382672\n",
      "2 Most imaging databases safe images permanently. 1.0514622242382672\n",
      "3 Imaging databases store data. 1.0\n",
      "4 Imaging databases store data. Imaging databases store data. Imaging databases store data. 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i, posts[i], dist(X[i], new_post_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "print(s.stem(\"imaging\"))\n",
    "print(s.stem(\"image\"))\n",
    "\n",
    "print(s.stem(\"imagination\"))\n",
    "print(s.stem(\"imagine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'safe', 'storag', 'store', 'stuff', 'toy']\n",
      "(5, 17)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.414213562373095\n",
      "1 Imaging databases provide storage capabilities. 0.8573732768944039\n",
      "2 Most imaging databases safe images permanently. 0.6296288974669553\n",
      "3 Imaging databases store data. 0.7653668647301795\n",
      "4 Imaging databases store data. Imaging databases store data. Imaging databases store data. 0.7653668647301795\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i, posts[i], dist(X[i], new_post_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF  詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'safe', 'storage', 'store', 'stuff', 'toy']\n",
      "(5, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "1 Imaging databases provide storage capabilities. 1.187009812033225\n",
      "2 Most imaging databases safe images permanently. 1.187009812033225\n",
      "3 Imaging databases store data. 1.091020922163783\n",
      "4 Imaging databases store data. Imaging databases store data. Imaging databases store data. 1.091020922163783\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i, posts[i], dist(X[i], new_post_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'safe', 'storag', 'store', 'stuff', 'toy']\n",
      "(5, 17)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
    "X = vectorizer.fit_transform(posts)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = 'imaging database'\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This is a toy post about machine learning. Actually, it contains not much interesting stuff. 1.4142135623730951\n",
      "1 Imaging databases provide storage capabilities. 1.0789758507558254\n",
      "2 Most imaging databases safe images permanently. 0.859044512133176\n",
      "3 Imaging databases store data. 0.924634506718001\n",
      "4 Imaging databases store data. Imaging databases store data. Imaging databases store data. 0.924634506718001\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(i, posts[i], dist(X[i], new_post_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文詞頻矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling jieba-0.39:\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba-0.39.dist-info/INSTALLER\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba-0.39.dist-info/METADATA\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba-0.39.dist-info/RECORD\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba-0.39.dist-info/WHEEL\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba-0.39.dist-info/top_level.txt\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/__init__.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/__init__.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/__main__.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/__pycache__/__init__.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/__pycache__/__main__.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/__pycache__/_compat.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/_compat.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/__init__.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/__init__.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/__pycache__/__init__.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/__pycache__/analyzer.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/__pycache__/textrank.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/__pycache__/tfidf.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/analyzer.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/analyzer.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/idf.txt\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/textrank.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/textrank.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/analyse/tfidf.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/dict.txt\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/__init__.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/__init__.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/__pycache__/__init__.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/__pycache__/prob_emit.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/__pycache__/prob_start.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/__pycache__/prob_trans.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_emit.p\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_emit.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_emit.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_start.p\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_start.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_start.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_trans.p\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_trans.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/finalseg/prob_trans.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__init__.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__init__.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__pycache__/__init__.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__pycache__/char_state_tab.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__pycache__/prob_emit.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__pycache__/prob_start.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__pycache__/prob_trans.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/__pycache__/viterbi.cpython-36.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/char_state_tab.p\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/char_state_tab.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/char_state_tab.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_emit.p\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_emit.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_emit.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_start.p\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_start.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_start.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_trans.p\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_trans.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/prob_trans.pyc\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/viterbi.py\n",
      "  /Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/jieba/posseg/viterbi.pyc\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 繁體中文版jieba\n",
    "- https://github.com/ldkrsi/jieba-zh_TW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "柯文哲\n",
      "為\n",
      "了\n",
      "大巨蛋\n",
      "一事\n",
      "找\n",
      "趙藤雄\n",
      "算帳\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict('userdict.txt')\n",
    "\n",
    "for w in jieba.cut('柯文哲為了大巨蛋一事找趙藤雄算帳'):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為 了 大巨蛋 一事 找 趙藤雄 算帳', '柯P 將不在 大巨蛋 舉辦 世運會']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['柯文哲為了大巨蛋一事找趙藤雄算帳', '柯P將不在大巨蛋舉辦世運會']\n",
    "\n",
    "courpus  = []\n",
    "for s in a:\n",
    "    courpus.append(' '.join(jieba.cut(s)))\n",
    "courpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['柯文哲 為 了 大巨蛋 一事 找 趙藤雄 算帳', '柯P 將不在 大巨蛋 舉辦 世運會']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [' '.join(jieba.cut(s)) for s in a]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '世運會', '大巨蛋', '將不在', '柯p', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 1 1 0 1]\n",
      " [0 1 1 1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist(v1, v2):\n",
    "    v1_normalized  = v1 / sp.linalg.norm(v1.toarray()) \n",
    "    v2_normalized  = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2649110640673518"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(X[0],X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "keyword = '郭雪芙'\n",
    "res = requests.get('https://zh.wikipedia.org/wiki/{}'.format(keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['郭雪芙', 'Puff Kuo']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym = []\n",
    "for p in soup.select('.mw-parser-output p'):\n",
    "    for b in p.select('b'):\n",
    "        synonym.append(b.text)\n",
    "synonym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## synonyms.txt\n",
    "- 柯文哲/柯P/柯p\n",
    "- 郭雪芙/puff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'puff': '郭雪芙', '柯p': '柯文哲'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dic = {}\n",
    "for s in open('synonyms.txt'):\n",
    "    synonym = s.strip().split('/')\n",
    "    for w in synonym[1:]:\n",
    "        synonym_dic[w.lower()]  = synonym[0]\n",
    "synonym_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '世運會', '大巨蛋', '將不在', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "\n",
    "class SynonymCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SynonymCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (synonym_dic.get(w, w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = SynonymCountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['世運會', '大巨蛋', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['為了', '一事', '將不在']\n",
    "\n",
    "vectorizer = SynonymCountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [ '2天才女童到澳洲深造 竟全身傷疑受虐',\n",
    "'來自台灣、現在只有9歲與7歲的2名兒童，上周六(13日)流落於澳洲街頭時被當地警方發現帶回安置，懷疑受虐和人口販運有關，隨即通知我國警方，不過經調查發現，這2名高智商的女童，其實是由父親委託友人帶去澳洲自主學習，父親表示並非遭到人口販運，小姐妹數理方面優異，當初委託朋友帶到澳洲深造。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(jieba.cut(s)) for s in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.681145747868608"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_raw(X[0], X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1,\n",
       "        1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=  ['我喜歡看電視不喜歡看電影','我不喜歡看電視也不喜歡看電影']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(jieba.cut(s)) for s in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我 喜歡 看 電視 不 喜歡 看 電影', '我 不 喜歡 看 電視 也 不 喜歡 看 電影']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ['柯文哲為了大巨蛋一事找趙藤雄算帳', '柯P將不在大巨蛋舉辦世運會']\n",
    "corpus = [' '.join(jieba.cut(s)) for s in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一事', '世運會', '大巨蛋', '將不在', '柯文哲', '算帳', '舉辦', '趙藤雄']\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "\n",
    "class SynonymCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SynonymCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (synonym_dic.get(w, w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = SynonymCountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0.4],\n",
       "       [0.4, 1. ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = cosine_similarity(X)\n",
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.6],\n",
       "       [0.6, 0. ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(X)\n",
    "cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新聞推薦系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/pytextmining/master/data/20171214news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "corpus = []\n",
    "titles = []\n",
    "for rec in news.iterrows():\n",
    "    #print(rec)\n",
    "    #break\n",
    "    corpus.append(' '.join(jieba.cut(rec[1].content)))\n",
    "    titles.append(rec[1].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'新增 ： 立委 說 法民進 黨 立法院 黨團 預計 在 明天 的 院會 中 ， 讓 改制 農田 水利 會 的 《 農田 水利 會 組織 通則 》 修正 草案 付委 ， 國民黨 主席 吳敦義 昨在 中常 會上 表示 ， 對 於 蔡 政府 這種 巧取 豪奪 水利 會資產 作法 要 全力 反對 ， 「 甚至 要 有 夜宿 立法院 的 準備 」 。 對此 ， 民進 黨 立委 蔡易餘 今天 早上 發臉 書 表示 ， 吳敦義的 軍令狀 已 經破功 了 ， 「 因為 我們 已 經先 佔 議場 前 ， 24 小時 固守 活動 開始 ！ ！ 」 民進 黨 立院 黨團書 記長 何欣純 今早 表示 ， 目前 排班 兩小時 一班 ， 夜晚 則由 男性 立委 負責 駐守 ， 不換班 。 民進 黨團 總召 柯建銘 今早 也 特別 叮嚀 綠委 ， 要 去 洗手 間 時 ， 必須 要 確認 一定 要 有 一個 人 駐守 在 議場 門口 ， 不能 全部 離開 。 今早 也 有 綠委說 ， 好險 今天 天氣 回暖 ， 坐在 這裡 沒有 問題 。 （ 黃 信維 ／ 台北 報導 ） 發稿 時間 ： 09 : 09 更新 時間 ： 09 : 23 \\xa0 想 知道 更 多 ， 一定 要 看 … … 國 防部 解約慶富 要 提告 \\u3000 馮世 寬 怒 嗆 三遍 ： 請 便 罷免 案將 投票 \\u3000 李遠 哲今 再度 現身力 挺 黃 國昌 【 歷史 上 的 今天 】 賴香伶當 選北市 勞動 局長 \\u3000 周錫瑋花 百萬蓋 縣長 辦 公室'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<899x38653 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128155 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cs = cosine_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899, 899)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.96829548, 0.94683527],\n",
       "       [0.96829548, 0.        , 0.9739099 ],\n",
       "       [0.94683527, 0.9739099 , 0.        ]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs[0:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 0, 4, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([80,70,56,63,89,90])\n",
    "a.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pos = cs[  0  ,  : ].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【更新】水利會改官派明闖關\\u3000綠委24小時前顧議場大門防藍突襲'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反對水利會改制　吳敦義下令：藍委做好夜宿立院抗爭準備 0.6321547504275353\n",
      "農田水利會改公務機關　蔡英文：這不是綁樁 0.6724128474406619\n",
      "罷免案將投票　李遠哲今再度現身力挺黃國昌 0.7771295373006972\n",
      "在野黨突襲表決《勞基法》修正案無效　民進黨烙人成功擋下 0.778385915154946\n",
      "【搏感情動畫】台電每年30億敦親睦鄰費　協助立委選民服務 0.7844869521946108\n",
      "國防部解約慶富要提告　馮世寬怒嗆三遍：請便 0.7946408559172016\n"
     ]
    }
   ],
   "source": [
    "for idx in sorted_pos[1:11]:\n",
    "    if cs[0,idx] < 0.8:\n",
    "        print(titles[idx], cs[0,idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimiliarArticle(pos):\n",
    "    print('查詢文章:',titles[pos])\n",
    "    sorted_pos = cs[  pos  ,  : ].argsort()\n",
    "    for idx in sorted_pos[1:11]:\n",
    "        if cs[pos,idx] < 0.8:\n",
    "            print('關聯文章:', titles[idx], cs[pos,idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢文章: 【更新】水利會改官派明闖關　綠委24小時前顧議場大門防藍突襲\n",
      "關聯文章: 反對水利會改制　吳敦義下令：藍委做好夜宿立院抗爭準備 0.6321547504275353\n",
      "關聯文章: 農田水利會改公務機關　蔡英文：這不是綁樁 0.6724128474406619\n",
      "關聯文章: 罷免案將投票　李遠哲今再度現身力挺黃國昌 0.7771295373006972\n",
      "關聯文章: 在野黨突襲表決《勞基法》修正案無效　民進黨烙人成功擋下 0.778385915154946\n",
      "關聯文章: 【搏感情動畫】台電每年30億敦親睦鄰費　協助立委選民服務 0.7844869521946108\n",
      "關聯文章: 國防部解約慶富要提告　馮世寬怒嗆三遍：請便 0.7946408559172016\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticle(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢文章: 黃國昌：長期目標要消滅國民黨　不支持柯文哲的兩岸一家親\n",
      "關聯文章: 「兩蔣時代」超譯　網友：不准傷皇城內的和氣！ 0.5791275247889435\n",
      "關聯文章: 罷免案將投票　李遠哲今再度現身力挺黃國昌 0.595349265179326\n",
      "關聯文章: 黃國昌若被罷免會更強　他：成為選輸北市的阿扁 0.6343677782259318\n",
      "關聯文章: 罷昌案周六投票　時代力量全力澄清不實謠言 0.6561575542619899\n",
      "關聯文章: 【聲援片】沈發惠呼籲罷昌案投「不同意」　黃國昌：謝謝您 0.7538388282723372\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticle(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢文章: 桃園工廠火勢熄滅　員工宿舍內發現一堆白骨\n",
      "關聯文章: 【不斷更新】桃園工廠惡火撲滅 6人仍失聯宿舍內發現一堆白骨 0.36731278412423196\n",
      "關聯文章: 汽車用品大廠「矽卡」燒毀　資本額達2億 0.5443494259494621\n",
      "關聯文章: 桃園工廠大火6員工失聯　家屬焦急等待 0.6699125279936771\n",
      "關聯文章: 醉女盧小小　女警帥爆！突然霸氣送她側摔　 0.7894412780969211\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticle(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章分群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/pytextmining/master/data/20150628news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>社會/生活</td>\n",
       "      <td>新北市八仙水上樂園昨晚發生粉塵爆炸，新北市衛生局統計到目前為止，由救護車送醫再加上自行送醫的...</td>\n",
       "      <td>Sun, 28 Jun 2015 07:40:00 +0800</td>\n",
       "      <td>八仙塵爆  五相關人依公共危險重傷害法辦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>社會/生活</td>\n",
       "      <td>新北市八仙樂園昨天晚上(6/27)舉辦活動，過程中噴灑大量玉米粉而引發粉塵爆炸，根據最新統計...</td>\n",
       "      <td>Sun, 28 Jun 2015 07:40:00 +0800</td>\n",
       "      <td>八仙樂園意外 病患持續增加中</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>財經/要聞</td>\n",
       "      <td>希臘債務違約限期逼近，資金持續外流。路透社引述三間銀行的消息指出，希臘國內有3分之1的自動櫃...</td>\n",
       "      <td>Sun, 28 Jun 2015 07:40:00 +0800</td>\n",
       "      <td>希臘國內三分一自動櫃員機現金短缺</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>社會/生活</td>\n",
       "      <td>台鐵「新基隆車站」第一階段工程本月底完成，台鐵表示，明天（29號）啟用後，由於南站周邊道路尚...</td>\n",
       "      <td>Sun, 28 Jun 2015 07:40:00 +0800</td>\n",
       "      <td>台鐵新基隆車站29日正式啟用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>財經/要聞</td>\n",
       "      <td>《中國時報》\\n●樂園變煉獄 派對驚爆 逾300人遭火紋身\\n八仙樂園昨晚舉辦「COLOR ...</td>\n",
       "      <td>Sun, 28 Jun 2015 07:38:17 +0800</td>\n",
       "      <td>6月28日各報頭版要聞</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                        description  \\\n",
       "0    社會/生活  新北市八仙水上樂園昨晚發生粉塵爆炸，新北市衛生局統計到目前為止，由救護車送醫再加上自行送醫的...   \n",
       "1    社會/生活  新北市八仙樂園昨天晚上(6/27)舉辦活動，過程中噴灑大量玉米粉而引發粉塵爆炸，根據最新統計...   \n",
       "2    財經/要聞  希臘債務違約限期逼近，資金持續外流。路透社引述三間銀行的消息指出，希臘國內有3分之1的自動櫃...   \n",
       "3    社會/生活  台鐵「新基隆車站」第一階段工程本月底完成，台鐵表示，明天（29號）啟用後，由於南站周邊道路尚...   \n",
       "4    財經/要聞  《中國時報》\\n●樂園變煉獄 派對驚爆 逾300人遭火紋身\\n八仙樂園昨晚舉辦「COLOR ...   \n",
       "\n",
       "                           pubdate                 title  \n",
       "0  Sun, 28 Jun 2015 07:40:00 +0800  八仙塵爆  五相關人依公共危險重傷害法辦  \n",
       "1  Sun, 28 Jun 2015 07:40:00 +0800        八仙樂園意外 病患持續增加中  \n",
       "2  Sun, 28 Jun 2015 07:40:00 +0800      希臘國內三分一自動櫃員機現金短缺  \n",
       "3  Sun, 28 Jun 2015 07:40:00 +0800        台鐵新基隆車站29日正式啟用  \n",
       "4  Sun, 28 Jun 2015 07:38:17 +0800           6月28日各報頭版要聞  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "titles = []\n",
    "for rec in news.iterrows():\n",
    "    corpus.append(' '.join(jieba.cut(rec[1].description)))\n",
    "    titles.append(rec[1].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<147x12825 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23783 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "n_cosine_similarities  = cosine_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "km = cluster.KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "c = km.fit_predict(n_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "希臘國內三分一自動櫃員機現金短缺\n",
      "歐元區財長拒希臘延長救助計劃\n",
      "呂紹煒專欄：違約與退出 希臘與歐洲才能重生(上)\n",
      "希臘違約在即  歐盟全力穩定經濟\n",
      "希臘脫歐變可能 歐洲衝擊大\n",
      "希債協議  法國願盡最後斡旋努力\n",
      "希臘1／3提款機錢被提光\n",
      "確保銀行穩定 希臘續與ECB緊密合作\n",
      "希臘態度強硬 歐元區耐心漸失\n",
      "希臘盼展延債務 歐元區拒絕\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "titles_ary = np.array(titles)\n",
    "for e in titles_ary[c==3]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "news = pandas.read_excel('https://raw.githubusercontent.com/ywchiu/pytextmining/master/data/20171214news.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "titles = []\n",
    "for rec in news.iterrows():\n",
    "    corpus.append(' '.join(jieba.cut(rec[1].content)))\n",
    "    titles.append(rec[1].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "n_cosine_similarities  = cosine_distances(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "km = cluster.KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "c = km.fit_predict(n_cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小嫻婚變冒毒菇　勾于美人「奉茶」夢魘\n",
      "明明是好康分享文　網友卻瘋喊噁心慎入\n",
      "「想抱孫要看天！」　他批小嫻婆婆殘忍強逼\n",
      "【好聚不好散】離婚掏出6千萬　這女星比小嫻還慘\n",
      "【狗仔偷拍】小嫻搬離何守正家租66坪房　月租6萬元\n",
      "許聖梅心疼小嫻被當空氣　爆何守正「有兩個女學員」\n",
      "【動畫解盤】毒菇跳火線譙seafood　小嫻難瘦香菇\n",
      "十二月十四日各報頭條搶先報\n",
      "不捨善良小嫻慘遭婚變　乃哥「命運捉弄人」\n",
      "【獨家】小嫻賣房求子　婆婆竟拒入籍何家\n",
      "【毒菇護弟】何守正姊姊不是華岡七仙女　美法連線批小嫻\n",
      "小嫻離婚導火線　拉何守正信妙禪\n",
      "【狗仔偷拍】何守正現身！「全台最沒尊嚴的婆婆」也出來了\n",
      "小嫻中分手魔咒！ 同公司4女星全都婚變\n",
      "他因為小嫻婚變被分手　網友跪求：拜託別放生\n",
      "潔哥目睹「正嫻之變」　驚呼：靠北系列竟是真的\n",
      "何守正姐姐神護航！遭網友酸：全台最討人厭大姑\n",
      "小嫻別傻傻被欺負！女律師說「姐寶」就要這樣對付\n",
      "【小嫻婚變】他說很奇怪　「一定是男生劈腿？」\n",
      "教友小嫻婚姻觸礁　曾之喬談情避不開Seafood\n",
      "小嫻守正結婚在台沒登記　想離婚只有兩條路\n",
      "胡瓜2個月前耳聞小嫻婚變　震驚之餘好心疼\n",
      "昔日搭檔談小嫻婚變　曾國城這樣說\n",
      "何守正兩個姊姊護航扯婆媳　「他」戳破媽寶特色\n",
      "大姑出面護弟！轟小嫻不能生「媽媽是全台最沒有尊嚴的婆婆」\n",
      "小嫻信奉妙禪　關鍵原因與何守正有關！\n",
      "小嫻何守正想離婚　必須先做這件事！\n",
      "【K律師論點】離婚＝失敗？　K律師這麼說\n",
      "真尷尬！他只是聊個天　正妹就退出對話了\n",
      "女網紅因為這理由挺余祥銓！讓人不知該哭還是笑\n",
      "「小嫻不快樂！」　許聖梅：何守正虧欠她\n",
      "一下車有人墜樓掉在車頂　網友：車牌有密碼\n",
      "【內幕動畫】小嫻婚變何守正姊反擊　不滿媽煮飯侍奉星媳婦\n",
      "小嫻多信妙禪？　曾見證「師父帶我跳舞」\n",
      "【話當年】被拍和她上賓館　何守正掰了阿妹\n",
      "小嫻婚變無徵兆　男星嘆：兩人向來出雙入對\n",
      "【獨家內幕】太傷！小嫻被分手　何守正當小三面前攤牌\n",
      "【小嫻離婚】何守正稱沒有遺憾　人妻女星超火「一嘴屁話」\n",
      "【有片】何守正小嫻驚爆離婚　健身房中嗅出端倪\n",
      "【小嫻離婚】3大退讓人財兩失　求子花光430萬積蓄\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "titles_ary = np.array(titles)\n",
    "for e in titles_ary[c==0]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil_ary = []\n",
    "for k in range(2,21):\n",
    "    km = cluster.KMeans(n_clusters = k, init='k-means++', random_state=42)\n",
    "    c = km.fit_predict(n_cosine_similarities)\n",
    "    sil_ary.append({'group':k,\n",
    "                    'silhouette':silhouette_score(n_cosine_similarities, labels=c)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'group': 2, 'silhouette': 0.6755654471937644},\n",
       " {'group': 3, 'silhouette': 0.15338953682280074},\n",
       " {'group': 4, 'silhouette': 0.0904743616163152},\n",
       " {'group': 5, 'silhouette': 0.09023331250993877},\n",
       " {'group': 6, 'silhouette': 0.09313344671208439},\n",
       " {'group': 7, 'silhouette': 0.09492595900171423},\n",
       " {'group': 8, 'silhouette': 0.08392193629479042},\n",
       " {'group': 9, 'silhouette': 0.08136447983549083},\n",
       " {'group': 10, 'silhouette': 0.08227220344756503},\n",
       " {'group': 11, 'silhouette': 0.08755001584378343},\n",
       " {'group': 12, 'silhouette': 0.08561332919793142},\n",
       " {'group': 13, 'silhouette': 0.08764196760874017},\n",
       " {'group': 14, 'silhouette': 0.08075272813877192},\n",
       " {'group': 15, 'silhouette': 0.0794831753988302},\n",
       " {'group': 16, 'silhouette': 0.08161712976054764},\n",
       " {'group': 17, 'silhouette': 0.08502936338670943},\n",
       " {'group': 18, 'silhouette': 0.09061744831829563},\n",
       " {'group': 19, 'silhouette': 0.0827405617617394},\n",
       " {'group': 20, 'silhouette': 0.08647846673217109}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sil_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>silhouette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.675565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.153390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.090474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.090233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.093133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.094926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.083922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.081364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.082272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.087550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.085613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.087642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.080753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.079483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>0.081617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.085029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.090617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>0.082741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>0.086478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group  silhouette\n",
       "0       2    0.675565\n",
       "1       3    0.153390\n",
       "2       4    0.090474\n",
       "3       5    0.090233\n",
       "4       6    0.093133\n",
       "5       7    0.094926\n",
       "6       8    0.083922\n",
       "7       9    0.081364\n",
       "8      10    0.082272\n",
       "9      11    0.087550\n",
       "10     12    0.085613\n",
       "11     13    0.087642\n",
       "12     14    0.080753\n",
       "13     15    0.079483\n",
       "14     16    0.081617\n",
       "15     17    0.085029\n",
       "16     18    0.090617\n",
       "17     19    0.082741\n",
       "18     20    0.086478"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "sil_df = pandas.DataFrame(sil_ary)\n",
    "sil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidchiu/.pyenv/versions/3.6.2/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['e', 'dist', 'rec']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1121d1c18>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% pylab inline\n",
    "sil_df.plot(kind = 'line', x='group', y='silhouette')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
